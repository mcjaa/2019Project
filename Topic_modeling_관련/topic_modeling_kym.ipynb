{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def get_csv_reader(filename, delimiter):\n",
    "    reader = []\n",
    "    if not os.path.isfile(filename):\n",
    "        csvfile = open(filename,\"w\")\n",
    "    else:\n",
    "        csvfile = open(filename, \"rt\")\n",
    "        reader = csv.DictReader(csvfile, delimiter=delimiter)\n",
    "        \n",
    "    return list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '유통/화장품 2Q19 Earnings Preview - 과도한 변동성에도 양호한 펀더멘탈은 존재', 'text': '? Top-picks\\n\\n- 유통: SK네트웍스, GS리테일, 현대홈쇼핑, 호텔신라\\n- 화장품: 클리오, 코스맥스, LG생활건강\\n\\n▶ 백화점, 가전전문점\\n\\n- 4월 백화점 기존점 감소에도 2분기 전체적으로 수입 명품 카테고리의 두 자리 수 고신장세 지속. 리빙, 식품, 아동스포츠, 롱패딩 등 여성복의 개선을 대신. 백화점 3사 기존점은 3~5% 예상.\\n- 가전판매(가전전문점) 2016년 이후 2018년 상반기까지 신규 가전 카테고리의 보급률 확대가 정점을 찍음. 특히 가전 카테고리는 2017년 하반기 수요 급증을 나타내며 2018년 3분기와 4분기 기저 부담 작용\\n? 면세점:\\n2019년 전자상거래법의 점진적 반영에도 상위 3사의 경쟁 우위도 독보적. 반면에 관세청의 국산 면세품 표시제 의무화 제시 등에 따른 불확실성 부담\\n? 할인점:\\n2분기 기존점 성장률 -5%~-2% 예상. 과도한 새벽 배송 경쟁에 따른 이마트의 2분기 영업적자 가능성 제시. 여기에 재산세 반영에 따른 고정비 부담\\n? 렌탈:\\n웅진코웨이의 재매각에 따른 단기 불확실성 부각에도 오히려 웅진그룹 리스크에 대한 확실한 제거 기대. 한편 SK네트웍스는 2분기 기존 사업에서의 실적 개선과 웅진코웨이 인수 관심으로 긍정적 센티먼트 확산\\n? 홈쇼핑:\\n소형가전, 렌탈 판매 호조와 패션 부문의 회복으로 취급고 증가 전망. IPTV송출수수료 인상은 여전히 20%~30% 수준이 불가피하지만 위성 및 케이블에서 하향 노력\\n? 편의점:\\n2019년 점포 순증 BGF리테일 700점(17년 1,600점대), GS리테일 700점 가까이로 마감. 공정위가 제시한 담배권 거리제한 영향으로 2019년 상위 편의점의 점포 순증도 500점 이하로 예상되는 반면 기존점의 개선도 가능할 전망.\\n▶ 화장품\\n\\n\\n? 클리오, LG생활건강:\\n\\nLG생활건강은 보수적인 가이던스를 유지함에도 2분기 3개 사업부의 안정적 영업으로 컨센서스 상회 가능 전망. 클리오는 1분기에 이어 2분기에도 국내외 핵심 채널에서 비타C세럼을 중심으로 히트상품 매출 호조세 유지\\n? 코스맥스, 한국콜마, 코스메카코리아:\\n홈쇼핑,온라인 고객사의 매출 증가로 로드샵 영향 상쇄, 해외 수출 회복 가시화 / 3사 모두 합병 법인의 연결 실적 본격화\\n? 아모레퍼시픽:\\n국내 로드샵에 대한 구조조정과 포맷 변화. 아리따움 라이브 전환 2분기 200개(1분기 10개, 2019년 300개F) 진행. 국내외 매출 둔화로 인한 고정비 부담 지속. 영업이익은 3년 연속 컨센서스 하회 추정. 2019년 글로벌 H&B채널 확대와 채널 별 브랜드 제고에 집중.\\n▶ 19.2Q 예상 실적 기준 우선 순위: 편의점, 홈쇼핑, 렌탈 > 면세점 > 화장품 > 전문점 > 할인점', 'url': 'https://www.ibks.com/index.do'}\n"
     ]
    }
   ],
   "source": [
    "#CSV파일을 dict의 객체형태로 읽어온다.(증권리포트 데이터 입력시사용)\n",
    "\n",
    "with open('report_test.csv','r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "   # for row in reader:\n",
    "    #    print(row)\n",
    "        \n",
    "article = {}\n",
    "article['title'] = row['title']\n",
    "article['text'] = row['text']\n",
    "article['url'] = row['url']\n",
    "\n",
    "print(article)\n",
    "articles = []\n",
    "for x in article:\n",
    "    articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('title', '유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검(김영민)'), ('text', '? 19.1H LVMH[MC FP] 연결(yoy) 매출액 15.3%, 순이익 9.2%\\n\\n? 화장품 부문 매출액 +9% yoy, 계속사업이익 +6% yoy (Organic Growth 기준)\\n\\n? 아시아, 미국, 유럽 지역 호조\\n(크리스챤 디올 꾸뛰르 실적 개선, LVMH: 향수, 화장품 플래그쉽브랜드 급속 성장, 세포라: 매장, 온라인 부문 매출 성장 강세, DFS(면세사업부문): 국제 관광객 증가에 기인)'), ('url', 'https://www.ibks.com/index.do')])\n"
     ]
    }
   ],
   "source": [
    "#증권리포트 분석을 위해 csv파일 읽어올때 사용하는 부분\n",
    "articles =[]\n",
    "articles = get_csv_reader(\"report_test.csv\",\",\")\n",
    "\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from newspaper import Article, ArticleException\n",
    "\n",
    "#기사 링크를 입력으로 받아 제목, 내용 그리고 링크를 파싱하여 dictionary 형태로 반환하는 함수\n",
    "#input - 기사 url\n",
    "#output - 기사의 dictionary 형태\n",
    "def crawl_newspaper(url):\n",
    "    \n",
    "    try:\n",
    "        article = Article(url,\n",
    "                          language='ko',\n",
    "                          memorize_articles=False,\n",
    "                          fetch_images=False,\n",
    "                          skip_bad_cleaner=True)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        article_data = {}\n",
    "        article_data['title'] = article.title.strip()  # 기사 제목\n",
    "        article_data['text'] = article.text.replace(\"\\n\", \" \").strip()  # 기사 내용에서 줄바꿈을 지우고, 좌우 공백 제거\n",
    "        article_data['url'] = url  # 기사 링크\n",
    "\n",
    "        return article_data\n",
    "    except ArticleException as AE:\n",
    "        raise AE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#네이버에서 검색어를 이용해 해당 기간 내의 뉴스 리스트를 반환하는 함수\n",
    "#input - 검색어, 기사일자, 뉴스리스트 시작페이지\n",
    "#output - 뉴스 리스트\n",
    "def get_newspaper_list(query,  # 검색어\n",
    "                       date=None,  # 기사 일자\n",
    "                       start_page=1  # 뉴스 리스트 페이지\n",
    "                      ):\n",
    "    \n",
    "    url = 'https://search.naver.com/search.naver?&where=news&query={query}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={start_date}&de={end_date}&start={start_page}'  # query와 기간을 조건으로 한 검색 url \n",
    "    if not date:\n",
    "        date = datetime.now()  # 오늘 날짜 instance 생성\n",
    "        date = date.strftime(\"%Y.%m.%d\")  # 날자 instance를 문자열 형태로 변환 ex) 2019.06.30\n",
    "    \n",
    "    res = requests.get(url.format(query=query, \n",
    "                                  start_date=date, \n",
    "                                  end_date=date, \n",
    "                                  start_page=(start_page - 1) * 10 + 1))\n",
    "    \n",
    "    # BeautifulSoup을 이용해 웹 html을 파싱\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    \n",
    "    # 최대 기사 페이지 수를 확인\n",
    "    pages = [tag.get_text() for tag in soup.select(\".paging > a, .paging > strong\")] # 이전페이지,1,2,3페이지\n",
    "    max_page = 0\n",
    "    for page in pages:\n",
    "        try:\n",
    "            page = int(page)\n",
    "            max_page = max(page, max_page)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if start_page > max_page:  # 더이상 페이지가 없을 경우 빈 리스트를 반환\n",
    "        return []\n",
    "    \n",
    "    # html 구조에서 해당 기사로 접근할 수 있는 url을 추출\n",
    "    link_list = [a['href'] for a in soup.select(\".news > ul.type01 > li > dl > dt > a\")]  \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from time import time\n",
    "\n",
    "#get_newspaper_list 함수를 이용하여 시작일자 ~ 끝일자까지의 뉴스리스트를 반환\n",
    "#input - 검색어 키워드, 검색 시기, 검색 종기, 검색 최대 페이지 수\n",
    "#output - 검색 시기~종기 기간의 검색된 뉴스리스트 반환\n",
    "\n",
    "def crawl_newspaper_by_query(query,  # 검색어 키워드\n",
    "                             start_date=None,\n",
    "                             end_date=None,\n",
    "                             max_page=10): #원래10 임시로 1\n",
    "    \n",
    "    \n",
    "    article_list = []  # 기사 리스트\n",
    "    crawled_url = set()  # 검색한 기사 중복 검색을 위한 변수\n",
    "        \n",
    "    if not start_date:\n",
    "        start_date = datetime.now()  # 오늘 날짜 instance 생성\n",
    "    else:\n",
    "        start_date = datetime.strptime(start_date, \"%Y.%m.%d\")  # 2019.06.30과 같은 문자열을 python instance로 변경\n",
    "        \n",
    "    if not end_date:\n",
    "        end_date = datetime.now()\n",
    "    else:\n",
    "        end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    \n",
    "    while start_date < end_date:\n",
    "        print(\"Date: \", end_date)\n",
    "        for cur_page in range(1, max_page):\n",
    "            print(\"Page: {}\".format(cur_page))\n",
    "            \n",
    "            # end_date 날짜의 기사 링크 수집\n",
    "            link_list = get_newspaper_list(query, end_date.strftime(\"%Y.%m.%d\"), start_page=cur_page)\n",
    "\n",
    "            if not link_list:  # 더이상 수집할 뉴스가 없을 경우\n",
    "                break\n",
    "            \n",
    "            for link in link_list:\n",
    "                # 중복 수집 방지\n",
    "                if link in crawled_url:\n",
    "                    continue\n",
    "                else:\n",
    "                    crawled_url.update([link])\n",
    "                \n",
    "                try:\n",
    "                    article = crawl_newspaper(link)\n",
    "                    article['date'] = start_date\n",
    "                    article_list.append(article)\n",
    "                except ArticleException as AE:\n",
    "                    continue\n",
    "        \n",
    "        end_date = end_date - timedelta(days=1)  # 수집할 날짜를 하루 전으로 변경\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  2019-07-26 00:00:00\n",
      "Page: 1\n",
      "5.47629976272583\n",
      "Total articles: 10\n",
      "==================== Article sample ====================\n",
      "{'title': '국내 유통된 중국산 조개젓서 A형간염 유전자 검출', 'text': '[아시아경제 오주연 기자] 중국산 조개젓에서 A형간염 바이러스 유전자가 검출됐다.  26일 질병관리본부에 따르면 인천시 보건환경연구원을 통해 중국에서 제조하고 국내에서 추가 가공한 조개젓에서 A형간염 바이러스 유전자가 검출됐다.  질병관리본부는 충남 소재 병원 종사자 6명에서 A형간염이 생긴 것을 인지하고 충청남도와 관할 보건소와 함께 현장 역학조사를 하는 과정에서 해당 조개젓이 병원 직원식당에 제공된 것을 확인하고 개봉하지 않은 조개젓 식품을 수거해 검사했다.  식품의약품안전처는 A형 간염바이러스 유전자가 검출된 해당 제품을 회수, 폐기하고 질병관리본부 등과 함께 환자와 식품과의 인과관계 등을 조사할 예정이다.  한편 조개젓에서 A형간염 바이러스 유전자가 검출된 것은 이번이 여섯 번째다.  A형 간염에 걸리면 발열, 오한, 오심, 구토, 황달 등의 증상이 있을 수 있다. 의심 증상이 나타나면 즉시 의사에게 진료를 받아야 한다.  오주연 기자 moon170@asiae.co.kr', 'url': 'https://view.asiae.co.kr/article/2019072619220973837', 'date': datetime.datetime(2019, 7, 25, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "#뉴스기사 크롤링 실행\n",
    "#input - 검색어, 검색시기, 검색종기, 검색할 최대페이지수\n",
    "#output - 검색된 뉴스리스트 articles에 저장\n",
    "#예시data articles[0] 출력\n",
    "start = time()\n",
    "articles = crawl_newspaper_by_query(\"유통\", \"2019.07.16\", \"2019.07.26\", max_page=10)\n",
    "print(time() - start)\n",
    "print(\"Total articles: {:,}\".format(len(articles)))\n",
    "print(\"=\"*20, \"Article sample\", \"=\"*20)\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd    #pandas를 불러온다.\n",
    "\n",
    "\n",
    "#크롤링 결과 results를 pandas의 DataFrame 형식으로 읽어온다.\n",
    "#data = pd.DataFrame(articles)\n",
    "#data.head()\n",
    "#data.to_csv('crawl_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n정규화 (normalization): 입니닼ㅋㅋ => 입니다 ㅋㅋ\\n어근화 (stemming): 한국어를 처리하는 예시입니다 => 한국어, 를, 처리, 하다, 예시, 이다\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#형태소 분류작업 (정규화, 어근화작업도 수행)\n",
    "#input - 형태소 분류할 문장\n",
    "#output - 분류된 형태소\n",
    "#예시data 출력\n",
    "#from konlpy.tag import Okt\n",
    "\n",
    "# https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/\n",
    "#tagger = Okt()  # Twitter 형태소 분석기 (Open Korean Text)\n",
    "\n",
    "#sentence = \"SK텔레콤-LG전자, 5G로 클라우드와 로봇 결합했다… 로봇산업 혁신 기대\"\n",
    "#words = tagger.pos(sentence, \n",
    "#                   norm=True,  # 정규화 - 입니다 ㄹㄹ -> 입니다로 모아주는\n",
    "#                   stem=True)  # 어근화 - 원형으로 바꾸는 작업\n",
    "#for w, t in words:\n",
    "#    print(w, t)\n",
    "\n",
    "    \n",
    "#\"\"\"\n",
    "#정규화 (normalization): 입니닼ㅋㅋ => 입니다 ㅋㅋ\n",
    "#어근화 (stemming): 한국어를 처리하는 예시입니다 => 한국어, 를, 처리, 하다, 예시, 이다\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#형태소 분류작업 (정규화, 어근화작업도 수행)\n",
    "#input - 형태소 분류할 문장\n",
    "#output - 분류된 형태소\n",
    "#예시data 출력\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "# https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/\n",
    "tagger = Okt()  # Twitter 형태소 분석기 (Open Korean Text)\n",
    "\n",
    "sentence = \"SK텔레콤-LG전자, 5G로 클라우드와 로봇 결합했다… 로봇산업 혁신 기대\"\n",
    "words = tagger.pos(sentence, \n",
    "                   norm=True,  # 정규화 - 입니다 ㄹㄹ -> 입니다로 모아주는\n",
    "                   stem=True)  # 어근화 - 원형으로 바꾸는 작업\n",
    "#for w, t in words:\n",
    "#    print(w, t)\n",
    "    \n",
    "\"\"\"\n",
    "정규화 (normalization): 입니닼ㅋㅋ => 입니다 ㅋㅋ\n",
    "어근화 (stemming): 한국어를 처리하는 예시입니다 => 한국어, 를, 처리, 하다, 예시, 이다\n",
    "\"\"\"\n",
    "\n",
    "#키워드 추출 조건 설정\n",
    "#input - 키워드로 포함하고자하는 품사 입력, 키워드로 포함하지 않을 금지단어 입력\n",
    "#output - 기사갯수 100단위로 출력, 시data 첫번째 기사항목 출력\n",
    "words_list = []\n",
    "vocab = Counter() # 글자를 숫자로 만들기 위함\n",
    "tag_set = set(['Noun', 'Alpha'])  # 키워드로 포함하고자 하는 품사 (명사, 알파벳)\n",
    "stopwords = set([\"글자\"])  # 키워드로 포함하지 않을 금지 단어 -> 하면서 포함되도록..\n",
    "\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    words = tagger.pos(article['text'], norm=True, stem=True)\n",
    "    tmp_words = []\n",
    "\n",
    "    for w, t in words:\n",
    "        if t in tag_set and len(w) > 1 and w not in stopwords:\n",
    "            tmp_words.append(w)\n",
    "    vocab.update(tmp_words)\n",
    "    words_list.append((tmp_words, article))\n",
    "    \n",
    "\n",
    "\n",
    "vocab = sorted([w for w, freq in vocab.most_common(10000)]) # voca size\n",
    "word2id = {w: i for i, w in enumerate(vocab)}  # {\"SK\": 0, \"로봇\": 1, \"딥러닝\": 2, ...}\n",
    "\n",
    "new_words_list = []\n",
    "for words, article in words_list:\n",
    "    words = [w for w in words if w in word2id]  # vocab에 있는 단어만 사용\n",
    "    \n",
    "    # 기사 내 서로 다른 단어 개수가 10개 이상인 기사만 사용    \n",
    "    if len(words) > 10:\n",
    "        new_words_list.append((words, article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1032\n",
      "Term document matrix shape: (10, 1032)\n",
      "  (0, 1022)\t0.0756664335641109\n",
      "  (0, 1019)\t0.0756664335641109\n",
      "  (0, 1015)\t0.06432338116935418\n",
      "  (0, 1012)\t0.05003282556053471\n",
      "  (0, 1010)\t0.06432338116935418\n",
      "  (0, 988)\t0.05627535822106181\n",
      "  (0, 976)\t0.10006565112106942\n",
      "  (0, 971)\t0.05003282556053471\n",
      "  (0, 942)\t0.0756664335641109\n",
      "  (0, 914)\t0.040619876393048086\n",
      "  (0, 878)\t0.0756664335641109\n",
      "  (0, 877)\t0.0756664335641109\n",
      "  (0, 866)\t0.0756664335641109\n",
      "  (0, 832)\t0.2269993006923327\n",
      "  (0, 826)\t0.0756664335641109\n",
      "  (0, 824)\t0.05003282556053471\n",
      "  (0, 806)\t0.1513328671282218\n",
      "  (0, 805)\t0.0756664335641109\n",
      "  (0, 797)\t0.0756664335641109\n",
      "  (0, 796)\t0.0756664335641109\n",
      "  (0, 780)\t0.0756664335641109\n",
      "  (0, 776)\t0.11255071644212362\n",
      "  (0, 774)\t0.3783321678205545\n",
      "  (0, 773)\t0.05627535822106181\n",
      "  (0, 772)\t0.0756664335641109\n",
      "  :\t:\n",
      "  (9, 91)\t0.11233668090522352\n",
      "  (9, 88)\t0.05136084690412453\n",
      "  (9, 87)\t0.025680423452062263\n",
      "  (9, 85)\t0.030209016063967187\n",
      "  (9, 73)\t0.030209016063967187\n",
      "  (9, 72)\t0.030209016063967187\n",
      "  (9, 69)\t0.030209016063967187\n",
      "  (9, 65)\t0.030209016063967187\n",
      "  (9, 63)\t0.030209016063967187\n",
      "  (9, 60)\t0.0799002865545786\n",
      "  (9, 57)\t0.022467336181044704\n",
      "  (9, 55)\t0.0674020085431341\n",
      "  (9, 54)\t0.030209016063967187\n",
      "  (9, 51)\t0.0674020085431341\n",
      "  (9, 46)\t0.025680423452062263\n",
      "  (9, 34)\t0.030209016063967187\n",
      "  (9, 33)\t0.025680423452062263\n",
      "  (9, 32)\t0.09062704819190157\n",
      "  (9, 28)\t0.05136084690412453\n",
      "  (9, 23)\t0.030209016063967187\n",
      "  (9, 18)\t0.030209016063967187\n",
      "  (9, 15)\t0.030209016063967187\n",
      "  (9, 13)\t0.030209016063967187\n",
      "  (9, 1)\t0.05136084690412453\n",
      "  (9, 0)\t0.12083606425586875\n"
     ]
    }
   ],
   "source": [
    "#input - words_list, vocab\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(len(words_list))\n",
    "print(len(vocab))\n",
    "term_doc_matrix = np.zeros((len(words_list), len(vocab)), dtype=np.float32) # 메트릭스 만듬\n",
    "for i, (words, article) in enumerate(new_words_list):\n",
    "    for word in words:\n",
    "        term_doc_matrix[i, word2id[word]] += 1\n",
    "        \n",
    "term_doc_matrix = TfidfTransformer().fit_transform(term_doc_matrix) #써이키런 -> 온갖 라이버러리 다 있다\n",
    "print(\"Term document matrix shape:\", term_doc_matrix.shape)\n",
    "print(term_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: (10, 5)\n",
      "H shape: (5, 1032)\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition\n",
    "from sklearn.decomposition import NMF\n",
    " \n",
    "K = 5  # topic 개수지정\n",
    "nmf = NMF(n_components=K, alpha=0.1)  # Non-negative matrix factorization\n",
    "W = nmf.fit_transform(term_doc_matrix) # 토픽모델링 해준다.\n",
    "H = nmf.components_\n",
    "\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"H shape:\", H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic\n",
      "리츠 점포 홈플러스 롯데 사업 매장 스페셜 온라인 부동산 상품 기존 사원 판촉 배송 물류 고객 통해 사장 이마트 마트 \n",
      " ====================\n",
      "1th topic\n",
      "화환 유통 통관 화훼 홍보 센터 aT 린지 위촉 올해 캠퍼스 뉴데일리 소비 식품 대사 월드 개발 교수 수출 한국 \n",
      " ====================\n",
      "2th topic\n",
      "간염 조개 유전자 바이러스 질병 본부 관리 식품 오주연 병원 증상 조사 해당 기자 추가 가공 인천 연구원 제조 역학 \n",
      " ====================\n",
      "3th topic\n",
      "퓨리 당질 전자랜드 일자리 딤채 스킨 마스크 LED 위니 푸르밀 본점 제품 환경 판매 IH 압력밥솥 대한민국 최초 조성 출시 \n",
      " ====================\n",
      "4th topic\n",
      "양파 마늘 이지은 장아찌 행사 직접 미래세 대인 농업인 농협 농협유통 엄용수 어린이 경남 수산 하나로마트 양재 이웃 로서 경상남도 \n",
      " ====================\n"
     ]
    }
   ],
   "source": [
    "for k in range(K):\n",
    "    print(\"{}th topic\".format(k))\n",
    "    for index in H[k].argsort()[::-1][:20]:\n",
    "        print(vocab[index], end=' ')\n",
    "        #vocabWrite[index] = vocab[index] + ' '\n",
    "        \n",
    "    print(\"\\n\", \"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1ef7182f6b10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnew_topic_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "new_topic_words = []\n",
    "\n",
    "for index in H[0].argsort()[::-1][:]:\n",
    "    print(vocab[index], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic\n",
      "유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ====================\n",
      "1th topic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검\n",
      "\n",
      " ====================\n",
      "2th topic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검\n",
      "\n",
      " ====================\n",
      "3th topic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검\n",
      "\n",
      " ====================\n",
      "4th topic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "유통/화장품 - 루이비통그룹(크리스찬디올) 19년 상반기 실적 점검\n",
      "\n",
      " ====================\n"
     ]
    }
   ],
   "source": [
    "for k in range(K): # 토픽(15)에 대해 관련 기사를 나열\n",
    "    print(\"{}th topic\".format(k))\n",
    "    for index in W[:, k].argsort()[::-1][:10]:\n",
    "        print(words_list[index][1]['title'])\n",
    "    print(\"\\n\", \"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: (6, 5)\n",
      "[t-SNE] Computing 5 nearest neighbors...\n",
      "[t-SNE] Indexed 6 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 6 samples in 0.001s...\n",
      "[t-SNE] Computed conditional probabilities for sample 6 / 6\n",
      "[t-SNE] Mean sigma: 1125899906842624.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: -112.601036\n",
      "[t-SNE] KL divergence after 1000 iterations: -13.810756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"W shape:\", W.shape)\n",
    "tsne = TSNE(n_components=2, init='pca', verbose=1) \n",
    "W2d = tsne.fit_transform(W)\n",
    "topic_index = [v.argmax() for v in W]  # 각각의 문서가 어느 topic에 속해 있는지 (hard clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('color', 6), ('document', 1), ('id', 6), ('topic', 6), ('x', 6), ('y', 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"646780c7-00c1-469e-a435-563665f44c08\" data-root-id=\"1002\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"fd80261c-76de-4277-a5e2-53b300b97344\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1011\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1016\",\"type\":\"LinearAxis\"}],\"plot_height\":580,\"plot_width\":720,\"renderers\":[{\"id\":\"1011\",\"type\":\"LinearAxis\"},{\"id\":\"1015\",\"type\":\"Grid\"},{\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"id\":\"1020\",\"type\":\"Grid\"},{\"id\":\"1028\",\"type\":\"BoxAnnotation\"},{\"id\":\"1049\",\"type\":\"Legend\"},{\"id\":\"1040\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1043\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1027\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1003\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1007\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1005\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1009\",\"type\":\"LinearScale\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1017\",\"type\":\"BasicTicker\"}},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\"},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1038\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"items\":[{\"id\":\"1050\",\"type\":\"LegendItem\"}],\"location\":\"top_left\",\"plot\":{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"1049\",\"type\":\"Legend\"},{\"attributes\":{\"label\":{\"field\":\"topic\"},\"renderers\":[{\"id\":\"1040\",\"type\":\"GlyphRenderer\"}]},\"id\":\"1050\",\"type\":\"LegendItem\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1039\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Topic\",\"@topic\"],[\"id\",\"@id\"],[\"Article\",\"@document\"]]},\"id\":\"1021\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1059\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"overlay\":{\"id\":\"1028\",\"type\":\"BoxAnnotation\"}},\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1058\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"data\":{\"color\":[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\"],\"document\":[\"\\uc720\\ud1b5/\\ud654\\uc7a5\\ud488 - \\ub8e8\\uc774\\ube44\\ud1b5\\uadf8\\ub8f9(\\ud06c\\ub9ac\\uc2a4\\ucc2c\\ub514\\uc62c) 19\\ub144 \\uc0c1\\ubc18\\uae30 \\uc2e4\\uc801 \\uc810\\uac80\"],\"id\":[0,1,2,3,4,5],\"topic\":[\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"],\"x\":{\"__ndarray__\":\"gjc0xmksEEVpLBBFaSwQRWksEEVpLBBF\",\"dtype\":\"float32\",\"shape\":[6]},\"y\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\",\"dtype\":\"float32\",\"shape\":[6]}},\"selected\":{\"id\":\"1058\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1059\",\"type\":\"UnionRenderers\"}},\"id\":\"1036\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1003\",\"type\":\"DataRange1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1021\",\"type\":\"HoverTool\"},{\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"SaveTool\"},{\"id\":\"1025\",\"type\":\"ResetTool\"},{\"id\":\"1026\",\"type\":\"WheelZoomTool\"}]},\"id\":\"1027\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1028\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1012\",\"type\":\"BasicTicker\"}},\"id\":\"1011\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1038\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\",\"type\":\"CDSView\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"1043\",\"type\":\"Title\"},{\"attributes\":{\"plot\":{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1012\",\"type\":\"BasicTicker\"}},\"id\":\"1015\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1017\",\"type\":\"BasicTicker\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.4\"}};\n",
       "  var render_items = [{\"docid\":\"fd80261c-76de-4277-a5e2-53b300b97344\",\"roots\":{\"1002\":\"646780c7-00c1-469e-a435-563665f44c08\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools_to_show = \"hover,box_zoom,pan,save,reset,wheel_zoom\"\n",
    "p = figure(plot_width=720, plot_height=580, tools=tools_to_show)\n",
    "\n",
    "source = ColumnDataSource(data={\n",
    "    'x': W2d[:, 0],\n",
    "    'y': W2d[:, 1],\n",
    "    'id': [i for i in range(W.shape[0])],\n",
    "    'document': [article['title'] for words, article in new_words_list],\n",
    "    'topic': [str(i) for i in topic_index],\n",
    "    'color': [Category20[K][i] for i in topic_index]\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "p.circle('x', 'y', source=source, legend='topic', color='color')\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "hover = p.select({'type': HoverTool})\n",
    "hover.tooltips = [('Topic', '@topic'), ('id', '@id'), ('Article', '@document')]\n",
    "hover.mode = 'mouse'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic과 Keyword 분포, 빈도를 확인할 수 있는 visualization\n",
    "#jupiter note에서 사용할때 그래프 확인후 interrupt the kernel로 종료 후 다음진행 필요\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def y_to_doc_topic(y):\n",
    "    n_topics = y.shape[1]\n",
    "    base = 1 / n_topics\n",
    "    doc_topic_prob = normalize(y, norm='l1')\n",
    "    rowsum = doc_topic_prob.sum(axis=1)\n",
    "    doc_topic_prob[np.where(rowsum == 0)[0]] = base\n",
    "    return doc_topic_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_to_topic_term(components):\n",
    "    n_terms = components.shape[1]\n",
    "    base = 1 / n_terms\n",
    "    topic_term_prob = normalize(components, norm='l1')\n",
    "    rowsum = topic_term_prob.sum(axis=1)\n",
    "    topic_term_prob[np.where(rowsum == 0)[0]] = base\n",
    "    return topic_term_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_prob = y_to_doc_topic(W)\n",
    "topic_term_prob = components_to_topic_term(H)\n",
    "doc_lengths = np.asarray(term_doc_matrix.sum(axis=1)).reshape(-1)\n",
    "term_frequency = np.asarray(term_doc_matrix.sum(axis=0)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 40)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-03fba2a2bf4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#display(prepared_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(data, ip, port, n_retries, local, open_browser, http_server, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepared_data_to_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m     serve(html, ip=ip, port=port, n_retries=n_retries, files=files,\n\u001b[0;32m    270\u001b[0m           open_browser=open_browser, http_server=http_server)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                            \u001b[0mvis_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         **kw).encode(obj)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyLDAvis\\utils.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    }
   ],
   "source": [
    "from pyLDAvis import prepare, show\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "\n",
    "print(topic_term_prob.shape)\n",
    "print(len(vocab))\n",
    "prepared_data = prepare(\n",
    "    topic_term_prob,\n",
    "    doc_topic_prob,\n",
    "    doc_lengths,\n",
    "    vocab,\n",
    "    term_frequency,\n",
    "    R = 10, #num of displayed terms,\n",
    "    sort_topics = False\n",
    ")\n",
    "\n",
    "#display(prepared_data)\n",
    "show(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siple search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쿼리_벡   (0, 0)\t0.015525654762480539\n",
      "  (3, 0)\t0.03038726997479271\n",
      "  (4, 0)\t0.047614238050700815\n",
      "  (5, 0)\t0.037838814854266684\n",
      "  (6, 0)\t0.07245710725661934\n",
      "  (8, 0)\t0.025748012811725146\n",
      "  (14, 0)\t0.021595244832185844\n",
      "  (15, 0)\t0.09776399273998984\n",
      "  (16, 0)\t0.02429739287382223\n",
      "  (18, 0)\t0.017462641751393698\n",
      "  (19, 0)\t0.01473374374899782\n",
      "  (20, 0)\t0.06411222382086657\n",
      "  (24, 0)\t0.21356202330529828\n",
      "  (30, 0)\t0.09302662522908386\n",
      "  (36, 0)\t0.05507677467104858\n",
      "  (37, 0)\t0.030252790532069262\n",
      "  (38, 0)\t0.04922516548982161\n",
      "  (40, 0)\t0.03068573869154204\n",
      "  (41, 0)\t0.07648189033898505\n",
      "  (43, 0)\t0.02224612903100561\n",
      "  (48, 0)\t0.030037437748755526\n",
      "  (49, 0)\t0.16509858674155767\n",
      "  (51, 0)\t0.013680973709323037\n",
      "  (52, 0)\t0.0198230950745455\n",
      "  (55, 0)\t0.03610805928367062\n",
      "  :\t:\n",
      "  (2378, 0)\t0.2680643708044099\n",
      "  (2381, 0)\t0.06229570752933079\n",
      "  (2383, 0)\t0.03808180434867973\n",
      "  (2384, 0)\t0.2443522077720839\n",
      "  (2386, 0)\t0.28721141887442675\n",
      "  (2387, 0)\t0.10444358604809643\n",
      "  (2390, 0)\t0.024845760774294696\n",
      "  (2391, 0)\t0.016695019073189828\n",
      "  (2394, 0)\t0.21548948182332098\n",
      "  (2395, 0)\t0.028328413711162796\n",
      "  (2399, 0)\t0.1306325987990424\n",
      "  (2405, 0)\t0.017516380466554564\n",
      "  (2406, 0)\t0.20716136686269693\n",
      "  (2407, 0)\t0.0160760293507647\n",
      "  (2408, 0)\t0.021658908230850146\n",
      "  (2409, 0)\t0.014652561017321817\n",
      "  (2410, 0)\t0.06584585340256167\n",
      "  (2411, 0)\t0.2689519086197474\n",
      "  (2413, 0)\t0.08061197425675425\n",
      "  (2427, 0)\t0.03138943595488611\n",
      "  (2445, 0)\t0.02024521601609854\n",
      "  (2448, 0)\t0.05955903730400605\n",
      "  (2450, 0)\t0.02218543256200813\n",
      "  (2461, 0)\t0.14749767771568026\n",
      "  (2462, 0)\t0.04234865673928224\n",
      "논제로_독 [   0    3    4    5    6    8   14   15   16   18   19   20   24   30\n",
      "   36   37   38   40   41   43   48   49   51   52   55   60   65   66\n",
      "   69   70   71   73   79   80   88   89   91   92   98  100  103  104\n",
      "  105  106  107  115  116  119  120  121  123  126  127  129  131  132\n",
      "  134  137  141  142  143  144  146  149  156  157  160  163  167  168\n",
      "  170  171  177  178  180  181  183  189  193  195  196  197  198  202\n",
      "  203  204  210  212  216  222  223  224  226  230  236  239  242  243\n",
      "  244  245  250  255  258  261  265  272  278  297  304  310  316  318\n",
      "  325  330  331  333  334  336  341  343  348  351  354  360  376  381\n",
      "  390  393  396  409  410  417  426  433  435  436  438  441  445  452\n",
      "  457  459  463  479  484  486  487  491  492  495  497  498  499  500\n",
      "  501  506  510  514  515  519  521  529  538  543  547  554  555  559\n",
      "  560  564  570  575  581  585  590  591  594  596  599  602  608  610\n",
      "  623  624  632  633  634  637  638  640  642  649  651  653  661  662\n",
      "  663  674  676  678  680  688  691  709  711  715  718  724  725  727\n",
      "  730  736  737  743  746  748  749  756  767  769  777  779  781  783\n",
      "  784  791  793  798  801  802  806  810  811  814  815  820  831  832\n",
      "  835  840  843  849  852  859  861  863  864  880  894  895  899  900\n",
      "  901  908  914  919  920  929  933  934  935  936  940  945  959  972\n",
      "  977  984  985  986  988  989  990  993  996 1001 1006 1010 1012 1013\n",
      " 1014 1018 1020 1024 1025 1039 1051 1060 1066 1069 1077 1079 1080 1081\n",
      " 1082 1088 1089 1090 1092 1101 1104 1107 1111 1116 1117 1118 1124 1125\n",
      " 1126 1130 1135 1137 1138 1143 1145 1148 1151 1160 1167 1170 1171 1173\n",
      " 1177 1182 1183 1186 1191 1193 1198 1207 1209 1219 1222 1234 1236 1237\n",
      " 1240 1243 1245 1249 1251 1252 1253 1254 1257 1258 1259 1260 1261 1262\n",
      " 1263 1264 1265 1266 1267 1269 1272 1273 1278 1283 1284 1285 1288 1289\n",
      " 1292 1293 1294 1296 1297 1298 1300 1301 1302 1304 1305 1306 1307 1311\n",
      " 1317 1321 1322 1323 1324 1325 1326 1328 1330 1333 1334 1335 1336 1337\n",
      " 1339 1341 1352 1355 1367 1371 1388 1396 1397 1400 1401 1403 1407 1414\n",
      " 1416 1421 1427 1428 1436 1444 1453 1455 1465 1468 1469 1474 1475 1483\n",
      " 1484 1495 1497 1498 1499 1500 1501 1504 1508 1510 1516 1517 1519 1520\n",
      " 1521 1523 1525 1527 1532 1533 1538 1541 1544 1547 1571 1575 1576 1588\n",
      " 1589 1596 1602 1617 1630 1634 1637 1638 1658 1662 1664 1667 1673 1674\n",
      " 1675 1677 1680 1683 1687 1697 1710 1713 1714 1715 1723 1728 1737 1740\n",
      " 1744 1747 1750 1752 1753 1755 1756 1760 1762 1771 1773 1774 1777 1780\n",
      " 1782 1792 1795 1797 1798 1799 1800 1802 1805 1806 1810 1812 1820 1821\n",
      " 1822 1828 1831 1832 1833 1842 1843 1844 1846 1851 1852 1853 1854 1856\n",
      " 1861 1862 1864 1865 1867 1868 1870 1872 1873 1874 1885 1886 1888 1896\n",
      " 1897 1899 1906 1913 1922 1925 1927 1931 1933 1938 1940 1941 1944 1949\n",
      " 1950 1951 1953 1957 1961 1965 1967 1971 1974 1975 1982 1988 1991 1992\n",
      " 2001 2002 2003 2004 2006 2008 2009 2012 2017 2032 2033 2034 2035 2037\n",
      " 2039 2040 2041 2044 2045 2047 2050 2052 2053 2054 2061 2064 2067 2078\n",
      " 2079 2082 2084 2085 2087 2102 2104 2111 2114 2118 2122 2123 2125 2128\n",
      " 2135 2136 2138 2143 2144 2145 2151 2157 2160 2171 2177 2180 2181 2184\n",
      " 2187 2190 2195 2200 2201 2211 2212 2214 2219 2222 2225 2226 2231 2245\n",
      " 2252 2254 2255 2257 2263 2264 2272 2274 2277 2278 2282 2285 2287 2294\n",
      " 2295 2296 2297 2303 2305 2311 2313 2320 2322 2324 2326 2333 2340 2344\n",
      " 2347 2351 2353 2354 2363 2371 2378 2381 2383 2384 2386 2387 2390 2391\n",
      " 2394 2395 2399 2405 2406 2407 2408 2409 2410 2411 2413 2427 2445 2448\n",
      " 2450 2461 2462]\n",
      "랭크 [1237 1634 2353 ... 1604 1605 1277]\n",
      "아시아 청년의 피땀눈물로 쌓은 ‘초일류 삼성’ http://www.hani.co.kr/arti/society/society_general/898252.html\n",
      "'삼바 수사'가 키운 불확실성…\"삼성전자 또 헤지펀드 공격받을 수도\" https://www.hankyung.com/article/2019061219931\n",
      "삼성 수뇌부 줄줄이 구속 위기…이재용 ‘투자 홍보’로 난관 돌파? http://www.hani.co.kr/arti/economy/economy_general/896486.html\n",
      "[다음주의 질문] 이재용 부회장은 지금 무엇을 해야 할까? / 곽정수 http://www.hani.co.kr/arti/economy/economy_general/897075.html\n",
      "[뉴스텔링] 이재용 삼성전자 부회장의 종횡무진 ‘이유 셋’ http://www.cnbnews.com/news/article.html?no=412032\n",
      "[종목썰쩐]올해 15% 상승한 삼성전자…\"화웨이 제재로 더 오른다\" https://www.hankyung.com/article/2019061155366\n",
      "삼성물산 달려간 이재용, 직원들과 점심 '식판회동' https://news.joins.com/article/olink/23097394\n",
      "[단독] 현대차, 차량 전장사업서 `반(反)삼성` 견제 배경은 http://www.dt.co.kr/contents.html?article_no=2019061802109932052003&ref=naver\n",
      "[단독] \"삼성, 화웨이처럼 당할 수도\"…경영진 연쇄 회의 나선 이재용 https://www.hankyung.com/article/2019060514191\n",
      "[2019 100대 CEO&기업] 김기남 부회장, 비메모리 반도체와 5G 통신 장비서 새 먹거리 찾는다 https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=050&aid=0000050755\n"
     ]
    }
   ],
   "source": [
    "query = '삼성'\n",
    "query_index = word2id[query]  # SKT 단어의 index\n",
    "\n",
    "query_vec = term_doc_matrix[:, query_index]  # 각 문서에서 SKT 단어에 해당하는 값을 추출\n",
    "nonzero_doc = query_vec.nonzero()[0]  # 0이 아닌 문서들만 사용\n",
    "print('논제로_독', nonzero_doc);\n",
    "ranks = query_vec.toarray().flatten().argsort()[::-1]\n",
    "print('랭크', ranks);\n",
    "\n",
    "for r in ranks[:10]:\n",
    "    if r in nonzero_doc:\n",
    "        print(new_words_list[r][1]['title'], new_words_list[r][1]['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
